# -*- coding: utf-8 -*-
"""NLP_final_model_report.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRts1Mid14h5G6dMNT90bzVVrz4fzY_4

With the dataset of the vectors (combined from the Tanach and Hebrew datasets) we have built, we will train two models, each for either Glinert (glirt in short) or Blau (blau) labels. We will preprocess the datasets accordingly (get rid of features we deem to be unecessary), conduct hyperparameter-tuning (calculated with 10-fold cross-validation) with a a variety of models and select the overall best models with the validation f1_score. Finally, we will test the models on the test dataset and calculate the f1_score and more.

# Imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif
from sklearn.decomposition import PCA

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, make_scorer, accuracy_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB

import warnings
from sklearn.exceptions import DataConversionWarning
import seaborn as sns
from tabulate import tabulate

###### FINAL #####
data = pd.read_csv(r'tagged_examples.csv')

# from google.colab import drive
# drive.mount('/content/drive')
# data = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/tagged_examples.csv')

#Separate the database to train and test randomly.

test_df = data.sample(frac=0.2, axis=0, random_state=0) # 0.6 to train

train_df = data.drop(index=test_df.index)


print(train_df.shape)
print(test_df.shape)

"""#Preprocessing

"""

data.keys()

"""Searching for features that can be safely removed (with zero standard deviation)"""

train_df_clean = train_df.drop(['Unnamed: 0', 'id', 'source', 'glirt_def', 'blau_def'], axis = 'columns')
test_df_clean = test_df.drop(['Unnamed: 0', 'id', 'source', 'glirt_def', 'blau_def'], axis = 'columns')


std = train_df_clean.std()

dup_columns = std[std == 0].index
if len(dup_columns) > 0:
    print(f"The are {len(dup_columns)} columns with zero standard deviation that can be safely removed:")
    for col in dup_columns:
        print(col)
else:
    print("There are no columns with a standard deviation of 0. Something cannot be automatically deleted.")


train_df_clean.drop(dup_columns, axis = 'columns', inplace=True)
test_df_clean.drop(dup_columns, axis = 'columns', inplace=True)

#create the dataframes for each target, moving the appropriate columns to the right of the df.

train_df_glirt = pd.concat([train_df_clean, train_df['glirt_def']], axis='columns')
train_df_blau = pd.concat([train_df_clean, train_df['blau_def']], axis='columns')

test_df_glirt = pd.concat([test_df_clean, test_df['glirt_def']], axis='columns')
test_df_blau = pd.concat([test_df_clean, test_df['blau_def']], axis='columns')

train_df_glirt.describe().drop(['count', '25%', '50%', '75%'], axis=0)

"""Let's count the occurences amount of each label and check for how balanced the training dataset is."""

fig, axs = plt.subplots(2, 2, figsize=(12, 8))
plt.subplots_adjust(wspace=0.3, hspace=0.5)

axs[0, 0].set_axis_off()

axs[0, 1].hist(train_df_glirt['glirt_def'], color='blue', alpha=0.5, label='target-glinert')

axs[0, 1].set_title('Glinert-labeled Dataset', fontweight='bold')
axs[0, 1].set_xlabel('Target')
axs[0, 1].set_ylabel('Count')
axs[0, 1].legend()

axs[1, 0].set_axis_off()

axs[1, 1].hist(train_df_blau['blau_def'], color='blue', alpha=0.5, label='target_blau')
axs[1, 1].set_title('Blau-labeled Dataset', fontweight='bold')
axs[1, 1].set_xlabel('Target')
axs[1, 1].set_ylabel('Count')
axs[1, 1].legend()

plt.tight_layout()

plt.show()

"""There is a noticeable imbalance for both of the labels.

We will now try and bring the amount of features even lower, we will analyze the dataset using pca, and proceed accordingly with a calculated guess.
"""

def plot_features_by(typ, string, feature_columns, vals):
  dup_df = pd.DataFrame({'Attribute': feature_columns,
                            f'Explained {string} Ratio': vals * 100})  # Convert to percentage
  dup_df[f'Cumulative {string} Ratio'] = np.cumsum(vals) * 100  # Convert to percentage
  dup_df[f'Explained {string} Ratio'] = dup_df[f'Explained {string} Ratio'].map('{:.6f}%'.format)
  dup_df[f'Cumulative {string} Ratio'] = dup_df[f'Cumulative {string} Ratio'].map('{:.6f}%'.format)
  plt.figure(figsize=(6, 4))
  plt.plot(range(1, len(feature_columns) + 1), vals * 100, marker='o', linestyle='-', label='Individual')
  plt.plot(range(1, len(feature_columns) + 1), np.cumsum(vals) * 100, marker='o', linestyle='--', label='Cumulative')
  plt.xlabel('Feature')
  plt.ylabel(f'Explained {string} Ratio (%)')
  plt.title(f'Explained {string} for {typ}')
  plt

  dup_df

feature_columns = np.array([col for col in train_df_glirt.columns if col != 'glirt_def'])
pca = PCA()
pca.fit(train_df_glirt[feature_columns])
explained_variance_ratio = pca.explained_variance_ratio_

plot_features_by("glinert/blau", "Variance", feature_columns, explained_variance_ratio)

"""The orange curve is the percentage of the overall 'string' (up to 100%) - which is overall explained variance in this graph - while the blue curve is the percentage of the individual variance-ratio (sorted from most to least important)"""

# Dividing the databases to matrices and label vectors

_X_train_glirt = train_df_glirt.drop(['glirt_def'], axis=1).to_numpy()
_y_train_glirt = train_df_glirt['glirt_def'].ravel()

_X_test_glirt = test_df_glirt.drop(['glirt_def'], axis=1).to_numpy()
_y_test_glirt = test_df_glirt['glirt_def'].ravel()

_X_train_blau = train_df_blau.drop(['blau_def'], axis=1).to_numpy()
_y_train_blau = train_df_blau['blau_def'].ravel()

_X_test_blau = test_df_blau.drop(['blau_def'], axis=1).to_numpy()
_y_test_blau = test_df_blau['blau_def'].ravel()

"""From the graph we may deduce that k=80 seems like a good number of features to keep based on the overall explained-variance contribution, but we want more evidence supporting that.

Fortunately, we can use the random forest model, which after fitting the data can output the "feature importance" of the training data - how important was each feature in the training phase of the ensemble model, built from multiple decision trees.
"""

glirt_rfc = RandomForestClassifier(random_state=0)
glirt_rfc.fit(_X_train_glirt, _y_train_glirt)

glirt_sorted_idx = np.argsort(glirt_rfc.feature_importances_)[::-1]
glirt_feature_idx = feature_columns[glirt_sorted_idx]
glirt_feature_imp = glirt_rfc.feature_importances_[glirt_sorted_idx]
plot_features_by("glinert", "feature importance", glirt_feature_idx, glirt_feature_imp)

blau_rfc = RandomForestClassifier(random_state=0)
blau_rfc.fit(_X_train_blau, _y_train_blau)

blau_sorted_idx = np.argsort(blau_rfc.feature_importances_)[::-1]
blau_feature_idx = feature_columns[blau_sorted_idx]
blau_feature_imp = blau_rfc.feature_importances_[blau_sorted_idx]
plot_features_by("blau", "feature importance", blau_feature_idx, blau_feature_imp)

"""The importance graph is very similar for both of the labels. If we were to value sheer accuracy, going for 100 features seems like a decent choice, but we also value processing time and for because of that we will try to strike a balance between processing time and not hurting the overall predictability too much.
Thus, we will keep 80 features for the glirt dataset, and 80 features for the blau dataset.

Now, we wouldn’t want to give the random forest model an advantage as we use it in the cross-validation stage, so we would use ‘SelectKBest’ as our final feature selection algorithm.
"""

# feature selection
k = 80

selector_dist = chi2
selector = SelectKBest(selector_dist, k=k)
selector.fit_transform(_X_train_glirt, _y_train_glirt)
glirt_feature_idx = selector.get_support(indices=True)

print(f"Best {k} features selected for glirt labeled df: ")
print(train_df_glirt.keys()[glirt_feature_idx].tolist())

selector = SelectKBest(selector_dist, k=k)
selector.fit_transform(_X_train_blau, _y_train_blau)
blau_feature_idx = selector.get_support(indices=True)

print(f"Best {k} features selected for blau labeled df: ")
print(train_df_blau.keys()[blau_feature_idx].tolist())


_X_train_glirt = _X_train_glirt[:, glirt_feature_idx]
_X_test_glirt = _X_test_glirt[:, glirt_feature_idx]

_X_train_blau = _X_train_blau[:, blau_feature_idx]
_X_test_blau = _X_test_blau[:, blau_feature_idx]

"""For curiosity's sake we also printed what the top 80 features will be based on feature importance:"""

k = 80
glirt_feature_idx = np.argsort(glirt_rfc.feature_importances_)[::-1][:k]

print(f"Best {k} features selected for glirt labeled df: ")
print(train_df_glirt.keys()[glirt_feature_idx].tolist())

blau_feature_idx = np.argsort(blau_rfc.feature_importances_)[::-1][:k]

print(f"Best {k} features selected for blau labeled df: ")
print(train_df_blau.keys()[blau_feature_idx].tolist())

"""# Hyperparameter tuning

We will find the best model according to each label.
There are two parts for each model training: the experiment and the testing.
We will start the experiment phase, and find the best hyperparameter(s) for each model using f1_score score.

**Then we will select the model that scored the highest at the experiment stage across all the models and their hyperparameters, and calculate the chosen model's full f_score (and more) in the testing phase (with the test datasets).**
"""

X_glirt = _X_train_glirt
X_blau = _X_train_blau

y_glirt = _y_train_glirt
y_blau = _y_train_blau

_cv_glirt = _cv_blau = 10

f1_scorer = make_scorer(f1_score , average='weighted')

def train_model_with_best_acc(X =X_glirt, y = y_glirt, cv = 10, model = DecisionTreeClassifier(), hyperparams = {}):
  gs = GridSearchCV(
      estimator = model,
      param_grid = hyperparams,
      scoring = f1_scorer,
      cv = cv,
      verbose = 4
  )
  gs.fit(X, y)
  return gs.cv_results_, gs.best_params_, gs.best_score_

def print_all_hparam_stats(df):
  for col in list(df.drop(columns="F1 score").columns):
    sns.lineplot(data=df, x=col, y='F1 score')
    plt.show()

"""## Support Vector (SVC)

A simple yet popular model, finds a separator vector with the best margin, will be used in our data with multiclass labels so it might not perform as well as the others.

With svc we have the following hyper parameters:
*   penalty - the norm, either l1 or l2.
*   loss - the loss function the model will be based on
*   C - a regularization parameter

Glirt:
"""

_hyperparams = {
               'degree' : [i for i in range(5)],
               'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],
               'C' : [0.001, 0.01, 0.1, 1, 10, 100, 100]
}

glirt_svc_res, glirt_svc_params, glirt_svc_score = train_model_with_best_acc(X = X_glirt, y = y_glirt, cv = _cv_glirt , model = SVC(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_svc_params)
print(f"\nBest model f1 score: {round((glirt_svc_score),3)}")

glirt_svc_hyp_stat = pd.concat([pd.DataFrame(glirt_svc_res["params"]),pd.DataFrame(glirt_svc_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_svc_hyp_stat)

"""Blau:"""

_hyperparams = {
               'degree' : [i for i in range(5)],
               'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],
               'C' : [0.001, 0.01, 0.1, 1, 10, 100, 100]
}
blau_svc_res, blau_svc_params, blau_svc_score = train_model_with_best_acc(X = X_blau, y = y_blau, cv = _cv_blau , model = SVC(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_svc_params)
print(f"\nBest model f1 score: {round((blau_svc_score),3)}")

blau_svc_hyp_stat = pd.concat([pd.DataFrame(blau_svc_res["params"]),pd.DataFrame(blau_svc_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_svc_hyp_stat)

"""##  K-nearest neighbors (KNN)

The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.

Search for hyperparameters on the training data that will give the best result:



*   n_neighbors - a parameter that determines the number of neighbors to consider when making predictions.
*   weights parameter - determines the weight assigned to each neighbor when making predictions
*   metric parameter - is responsible for defining the distance metric used to measure the similarity or dissimilarity between data points.

Glirt:
"""

_hyperparams = {
               'n_neighbors' : [2*i+1 for i in range(20)],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']
}
glirt_knn_res, glirt_knn_params, glirt_knn_score = train_model_with_best_acc(X = X_glirt, y = y_glirt, cv = _cv_glirt , model = KNeighborsClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_knn_params)
print(f"\nBest model f1 score: {round((glirt_knn_score),3)}")

"""Hyper parameter statistics:"""

glirt_knn_hyp_stat = pd.concat([pd.DataFrame(glirt_knn_res["params"]),pd.DataFrame(glirt_knn_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_knn_hyp_stat)

"""Rosen:"""

_hyperparams = {
               'n_neighbors' : [2*i+1 for i in range(20)],
               'weights' : ['uniform','distance'],
               'metric' : ['minkowski','euclidean','manhattan']
}
blau_knn_res, blau_knn_params, blau_knn_score = train_model_with_best_acc(X = X_blau, y = y_blau, cv = _cv_blau, model = KNeighborsClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_knn_params)
print(f"\nBest model f1 score: {round((blau_knn_score),3)}")

blau_knn_hyp_stat = pd.concat([pd.DataFrame(blau_knn_res["params"]),pd.DataFrame(blau_knn_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_knn_hyp_stat)

"""## Naive Bayes (Multinomial)

This model assumes that the features are mutually exclusive, which is not the case in our data. Thus, we don't expect this model to win. Using it in our best-model searching is still interesting enough to give it a try.

hyperparameters:

* alpha
* force_alpha

Glirt:
"""

_hyperparams = {
               'alpha' : [0.00001 * (10 ** i) for i in range(10)],
               'force_alpha' : [True, False],
               'fit_prior' : [True, False]
}

glirt_nb_res, glirt_nb_params, glirt_nb_score = train_model_with_best_acc(X = X_glirt, y = y_glirt, cv = _cv_glirt , model = MultinomialNB(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_nb_params)
print(f"\nBest model f1 score: {round((glirt_nb_score),3)}")

glirt_nb_hyp_stat = pd.concat([pd.DataFrame(glirt_nb_res["params"]),pd.DataFrame(glirt_nb_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_nb_hyp_stat)

"""Blau:"""

_hyperparams = {
               'alpha' : [0.00001 * (10 ** i) for i in range(10)],
               'force_alpha' : [True, False],
               'fit_prior' : [True, False]
}

blau_nb_res, blau_nb_params, blau_nb_score = train_model_with_best_acc(X = X_blau, y = y_blau, cv = _cv_blau , model = MultinomialNB(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_nb_params)
print(f"\nBest model f1 score: {round((blau_nb_score),3)}")

blau_nb_hyp_stat = pd.concat([pd.DataFrame(blau_nb_res["params"]),pd.DataFrame(blau_nb_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_nb_hyp_stat)

"""## Logistic Regression

A model that is based on linear regression, with the use of thresholds to transform a regression problem/solution into a classification one.
Search for hyperparameters on the balanced data that will give the best result:



*   Solver - the algorithm to use in the optimization problem.
*   C - (or regularization strength) Regularization strength works with the penalty to regulate overfitting.
*   max_iter - The maximum number of
iterations for the solver to converge.

Glirt:
"""

_hyperparams = {
    "solver": ["lbfgs", "liblinear", "newton-cg", "sag", "saga"], #default=’lbfgs’
    "C": [0.1*(i+1) for i in range(20)]
}
glirt_regr_res, glirt_regr_params, glirt_regr_score = train_model_with_best_acc(X =X_glirt, y = y_glirt, model = LogisticRegression(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_regr_params)
print(f"\nBest model f1 score: {round((glirt_regr_score),3)}")

glirt_reg_hyp_stat = pd.concat([pd.DataFrame(glirt_regr_res["params"]),pd.DataFrame(glirt_regr_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_reg_hyp_stat)

"""Blau:"""

_hyperparams = {
    "solver": ["lbfgs", "liblinear", "newton-cg", "sag", "saga"], #default=’lbfgs’
    "C": [0.1*(i+1) for i in range(20)]
}
blau_regr_res, blau_regr_params, blau_regr_score = train_model_with_best_acc(X =X_blau, y = y_blau , cv = _cv_blau, model = LogisticRegression(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_regr_params)
print(f"\nBest model f1 score: {round((blau_regr_score),3)}")

blau_reg_hyp_stat = pd.concat([pd.DataFrame(blau_regr_res["params"]),pd.DataFrame(blau_regr_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_reg_hyp_stat)

"""##Decision Tree

While we do not expect a great accuracy from a decision tree model, the advantage of said model is it's straight-forward build and performance in comparison with the other "tree based" models we will use later on. With that reasoning we will opt for trying more combinations of hyper parameters.

With decision tree mode, we have 3 hyper parameters:
* criterion - calculating impurity
* max_depth - of the tree
* min_samples_leaf - minimum amount of samples in a vertex in order to convert it to a leaf

Glirt:
"""

_hyperparams = {
    "criterion" : ["gini", "entropy"],
    "max_depth" : [i+1 for i in range(21)], # 2 ^ 20 > 2 * 65536 > _X.shape[0]
    "min_samples_leaf" : [(i+1) for i in range(70)]
}

glirt_dec_res, glirt_dec_params, glirt_dec_score = train_model_with_best_acc(X =X_glirt, y = y_glirt, model = DecisionTreeClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_dec_params)
print(f"\nBest model balanced f1 score: {glirt_dec_score}")

glirt_dec_res

glirt_dec_hyp_stat = pd.concat([pd.DataFrame(glirt_dec_res["params"]),pd.DataFrame(glirt_dec_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_dec_hyp_stat)

"""Blau:"""

_hyperparams = {
    "criterion" : ["gini", "entropy"],
    "max_depth" : [i+1 for i in range(21)], # 2 ^ 20 > 2 * 65536 > _X.shape[0]
    "min_samples_leaf" : [(i+1) for i in range(70)]
}
blau_dec_res, blau_dec_params, blau_dec_score = train_model_with_best_acc(X =X_blau, y = y_blau, cv = _cv_blau, model = DecisionTreeClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_dec_params)
print(f"\nBest model balanced f1 score: {blau_dec_score}")

blau_dec_hyp_stat = pd.concat([pd.DataFrame(blau_dec_res["params"]),pd.DataFrame(blau_dec_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_dec_hyp_stat)

"""## Random Forest

How about using the previous model concept, only with many trees instead? That is the idea behind the ensemble model that is random forest classifier.

With random forest regressor we have 4 hyperparameters:

* n_estimators - the number of decision trees the model will use. it will base the predicted label based on majority vote.
* criterion
* max_depth
* min_samples_leaf

Glirt:
"""

_hyperparams = {
    "n_estimators" : [2] + [(i+1)*25 for  i in range(4)],
    "criterion" : ['gini', 'entropy', 'log_loss'],
    "max_depth" : [2*i+1 for i in range(10)],
    "min_samples_leaf" : [i+1 for i in range(10)],
    "random_state" : [5]
}
glirt_rfc_res, glirt_rfc_params, glirt_rfc_score = train_model_with_best_acc(X =X_glirt, y = y_glirt, cv = _cv_glirt, model = RandomForestClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_rfc_params)
print(f"\nBest model balanced f1 score: {glirt_rfc_score}")

glirt_rfc_hyp_stat = pd.concat([pd.DataFrame(glirt_rfc_res["params"]),pd.DataFrame(glirt_rfc_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_rfc_hyp_stat)

"""Blau:"""

_hyperparams = {
    "n_estimators" : [2] + [(i+1)*25 for  i in range(4)],
    "criterion" : ['gini', 'entropy', 'log_loss'],
    "max_depth" : [2*i+1 for i in range(10)],
    "min_samples_leaf" : [i+1 for i in range(10)],
    "random_state" : [5]
}
blau_rfc_res, blau_rfc_params, blau_rfc_score = train_model_with_best_acc(X =X_blau, y = y_blau, cv = _cv_blau, model = RandomForestClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_rfc_params)
print(f"\nBest model f1 score: {blau_rfc_score}")

blau_rfc_hyp_stat = pd.concat([pd.DataFrame(blau_rfc_res["params"]),pd.DataFrame(blau_rfc_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_rfc_hyp_stat)

"""##AdaBoost

A bit more advanced method than random forest - ada boost builds multiple trees - where each tree will contribute an "ammount of say". The label with the most "ammount of say" will be chosen for each sentence vector during the validation stage.

We will keep the "atomic" estimator that adaboost is based on, as decision tree with depth of 1.

With adaboost we have 2 hyperparameters:

* n_estimators - the amount of weak learners
* learning_rate - controls the amount of contribution of each weak learner in the prediction.

Glirt:
"""

_hyperparams = {
    "n_estimators" : [((i*25)+25) for i in range(11)],
    "learning_rate" : [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1]
}
glirt_ada_res, glirt_ada_params, glirt_ada_score = train_model_with_best_acc(X = X_glirt, y = y_glirt, model = AdaBoostClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(glirt_ada_params)
print(f"\nBest model f1 score: {glirt_ada_score}")

glirt_ada_hyp_stat = pd.concat([pd.DataFrame(glirt_ada_res["params"]),pd.DataFrame(glirt_ada_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(glirt_ada_hyp_stat)

"""Blau:"""

_hyperparams = {
    "n_estimators" : [((i*25)+25) for i in range(11)],
    "learning_rate" : [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.5, 1]
}
blau_ada_res, blau_ada_params, blau_ada_score = train_model_with_best_acc(X = X_blau, y = y_blau, cv = _cv_blau, model = AdaBoostClassifier(), hyperparams = _hyperparams)

print(f"Best model parameters: ")
print(blau_ada_params)
print(f"\nBest model f1 score: {blau_ada_score}")

blau_ada_hyp_stat = pd.concat([pd.DataFrame(blau_ada_res["params"]),pd.DataFrame(blau_ada_res["mean_test_score"], columns=["F1 score"])],axis=1)
print_all_hparam_stats(blau_ada_hyp_stat)

"""#Final models selection

Glirt:
"""

glirt_values = [glirt_svc_score, glirt_knn_score, glirt_nb_score, glirt_regr_score, glirt_dec_score, glirt_rfc_score, glirt_ada_score]
glirt_f1_score_df = {
    'Model': ['SVC', 'KNN', 'Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost'],
    'F1 Score': glirt_values
}

df = pd.DataFrame(glirt_f1_score_df)
df['F1 Score'] = (df['F1 Score']).astype(str)
table = tabulate(df, headers='keys', tablefmt='psql', showindex=False)
print("\t\tGlinert models table")
print(table)

print(f"The model selected for glinert label is {glirt_f1_score_df['Model'][np.argmax(glirt_values)]}.")

"""Blau:"""

blau_values = [blau_svc_score, blau_knn_score, blau_nb_score, blau_regr_score, blau_dec_score, blau_rfc_score, blau_ada_score]
blau_f1_score_df = {
    'Model': ['SVC', 'KNN', 'Naive Bayes', 'Logistic Regression', 'Decision Tree', 'Random Forest', 'AdaBoost'],
    'F1 Score': blau_values
}
df = pd.DataFrame(blau_f1_score_df)
df['F1 Score'] = (df['F1 Score']).astype(str)
table = tabulate(df, headers='keys', tablefmt='psql', showindex=False)
print("\t\Blau models table")
print(table)

print(f"The model selected for blau label is {blau_f1_score_df['Model'][np.argmax(blau_values)]}.")

"""# Testing

"""

def get_best_model_glirt(): # generic function that will return the best model of the experiment stage, for the glirent label.
  i = np.argmax(glirt_values) # best f_score we received overall from cross validation in glirt dataset
  if i == 0:
    return SVC(C=glirt_svc_params['C'], degree=glirt_svc_params['degree'], kernel=glirt_svc_params['kernel'])
  elif i == 1:
    return KNeighborsClassifier(metric=glirt_knn_params['metric'], n_neighbors=glirt_knn_params['n_neighbors'], weights=glirt_knn_params['weights'])
  elif i == 2:
    return MultinomialNB(alpha = glirt_nb_params['alpha'], fit_prior = glirt_nb_params['fit_prior'], force_alpha = glirt_nb_params['force_alpha'])
  elif i == 3:
    return LogisticRegression(solver = glirt_regr_params['solver'], C = glirt_regr_params['C'])
  elif i == 4:
    return DecisionTreeClassifier(criterion=glirt_dec_params['criterion'], max_depth=glirt_dec_params['max_depth'], min_samples_leaf=glirt_dec_params['min_samples_leaf'])
  elif i == 5:
    return RandomForestClassifier(criterion=glirt_rfc_params['criterion'], max_depth=glirt_rfc_params['max_depth'], min_samples_leaf=glirt_rfc_params['min_samples_leaf'], n_estimators=glirt_rfc_params['n_estimators'],random_state=glirt_rfc_params['random_state'])
  else :
    return AdaBoostClassifier(learning_rate = glirt_ada_params['learning_rate'], n_estimators=glirt_ada_params['n_estimators'])

def get_best_model_blau():
  i = np.argmax(blau_values)
  if i == 0:
    return SVC(C=blau_svc_params['C'], degree=blau_svc_params['degree'], kernel=blau_svc_params['kernel'])
  elif i == 1:
    return KNeighborsClassifier(metric=blau_knn_params['metric'], n_neighbors=blau_knn_params['n_neighbors'], weights=blau_knn_params['weights'])
  elif i == 2:
    return MultinomialNB(alpha = blau_nb_params['alpha'], fit_prior = blau_nb_params['fit_prior'], force_alpha = blau_nb_params['force_alpha'])
  elif i == 3:
    return LogisticRegression(solver = blau_regr_params['solver'], C = blau_regr_params['C'])
  elif i == 4:
    return DecisionTreeClassifier(criterion=blau_dec_params['criterion'], max_depth=blau_dec_params['max_depth'], min_samples_leaf=blau_dec_params['min_samples_leaf'])
  elif i == 5:
    return RandomForestClassifier(criterion=blau_rfc_params['criterion'], max_depth=blau_rfc_params['max_depth'], min_samples_leaf=blau_rfc_params['min_samples_leaf'], n_estimators=blau_rfc_params['n_estimators'],random_state=blau_rfc_params['random_state'])
  else :
    return AdaBoostClassifier(learning_rate = blau_ada_params['learning_rate'], n_estimators=blau_ada_params['n_estimators'])

"""We see that for glinert classification, the Decision-Tree model has the best accuracy score on the validation dataset in comparison to the rest of the models.

Now we may train the chosen model (with the best hyperparameters) on the original dataset and check the accuracy on the test.
"""

final_glirt_model = get_best_model_glirt()

final_glirt_model.fit(X_glirt, y_glirt)
y_test_glirt_predict = final_glirt_model.predict(_X_test_glirt)
final_glirt_f = f1_score(_y_test_glirt, y_test_glirt_predict, average='weighted')
final_glirt_overall = f1_score(_y_test_glirt, y_test_glirt_predict, average=None)

print(f'Test F1 Weighted Score for glirent: {final_glirt_f}')
print(f'Test F1 Overall Score for glirent: {final_glirt_overall}')

"""and for blau, Decision-Tree was chosen:"""

final_blau_model = get_best_model_blau()

final_blau_model.fit(X_blau, y_blau)
final_glirt_model.fit(X_blau, y_blau)
y_test_blau_predict = final_blau_model.predict(_X_test_blau)
final_blau_f = f1_score(_y_test_blau, y_test_blau_predict, average='weighted')
final_blau_overall = f1_score(_y_test_blau, y_test_blau_predict, average=None)

print(f'Test F1 Weighted Score for blau: {final_blau_f}')
print(f'Test F1 Overall Score for blau: {final_blau_overall}')

test_df["glirt_def_predict"] = final_glirt_model.predict(_X_test_glirt)
test_df["blau_def_predict"] = final_blau_model.predict(_X_test_blau)

bible_test_df = test_df.query("source == 'Bible'")
modern_heb_test_df = test_df.query("source == 'modernHebrew'")

"""### **Glinert Accuracy**"""

glirt_bible_accuracy = len(bible_test_df[bible_test_df["glirt_def_predict"] == bible_test_df["glirt_def"]])/len(bible_test_df)
glirt_modern_heb_accuracy = len(modern_heb_test_df[modern_heb_test_df["glirt_def_predict"] == modern_heb_test_df["glirt_def"]])/len(modern_heb_test_df)
print("Modern Hebrew accuracy is: "+str(glirt_modern_heb_accuracy))
print("Bible accuracy is: "+str(glirt_bible_accuracy))
print("Total glirt Accuracy: " + str(len(test_df[test_df["glirt_def"] == test_df["glirt_def_predict"]])/len(test_df)))

"""### **Blau Accuracy**"""

blau_bible_accuracy = len(bible_test_df[bible_test_df["blau_def_predict"] == bible_test_df["blau_def"]])/len(bible_test_df)
blau_modern_heb_accuracy = len(modern_heb_test_df[modern_heb_test_df["blau_def_predict"] == modern_heb_test_df["blau_def"]])/len(modern_heb_test_df)
print("Modern Hebrew accuracy is: "+str(blau_modern_heb_accuracy))
print("Bible accuracy is: "+str(blau_bible_accuracy))
print("Total Rosen Accuracy: " + str(len(test_df[test_df["blau_def"] == test_df["blau_def_predict"]])/len(test_df)))

"""### **The Models**"""

final_glirt_model

final_blau_model
